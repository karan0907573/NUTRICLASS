{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab5b6d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26589fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=Path(\"../Data/cleaned_food_data.csv\")\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# --- STEP 1: Outlier Clipping ---\n",
    "# We identify numerical nutrient columns to clip extreme noise/errors\n",
    "numeric_cols = ['Calories', 'Protein', 'Fat', 'Carbs', 'Sugar', 'Fiber', \n",
    "                'Sodium', 'Cholesterol', 'Glycemic_Index', 'Water_Content', 'Serving_Size']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    # We find the 1st and 99th percentile\n",
    "    lower_limit = df[col].quantile(0.01)\n",
    "    upper_limit = df[col].quantile(0.99)\n",
    "    \n",
    "    # Clip values: anything below 1% or above 99% is capped\n",
    "    df[col] = df[col].clip(lower=lower_limit, upper=upper_limit)\n",
    "\n",
    "print(\"Outlier clipping complete (1st and 99th percentiles).\")\n",
    "\n",
    "# --- STEP 2: Boolean Encoding ---\n",
    "df['Is_Vegan'] = df['Is_Vegan'].astype(int)\n",
    "df['Is_Gluten_Free'] = df['Is_Gluten_Free'].astype(int)\n",
    "\n",
    "# --- STEP 3: One-Hot Encoding ---\n",
    "# We use drop_first=True to reduce redundancy (n-1 columns)\n",
    "df = pd.get_dummies(df, columns=['Meal_Type', 'Preparation_Method'], prefix=['Meal', 'Prep'])\n",
    "\n",
    "# --- STEP 4: Target Label Encoding ---\n",
    "le = LabelEncoder()\n",
    "df['Food_Name_Encoded'] = le.fit_transform(df['Food_Name'])\n",
    "food_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(\"\\nTarget Mapping:\", food_mapping)\n",
    "\n",
    "# --- STEP 5: Feature Scaling ---\n",
    "# We do this AFTER clipping so the outliers don't shrink the normal data\n",
    "scaler = StandardScaler()\n",
    "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "# --- STEP 6: Final Cleanup & Save ---\n",
    "df_final = df.drop(columns=['Food_Name'])\n",
    "df_final.to_csv('feature_engineered_data.csv', index=False)\n",
    "\n",
    "print(\"\\nFeature Engineering with Clipping is Complete!\")\n",
    "print(f\"Final dataset shape: {df_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aeb72319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.770338410762577\n",
      "508.7917362181856\n",
      "-9.375239925850387\n",
      "25.887437541037706\n",
      "-3.0930711189964555\n",
      "27.105562312982748\n",
      "10.808238415650232\n",
      "48.85904698000744\n",
      "-12.237284775750542\n",
      "30.354091315894102\n",
      "-2.14101988488381\n",
      "5.5420230941989015\n",
      "-788.8343365681962\n",
      "1482.1397250856407\n",
      "-20.91326701688886\n",
      "68.57298949603188\n",
      "36.76613932649788\n",
      "98.56851477440715\n",
      "18.96321616105986\n",
      "72.39139920661246\n",
      "-9.940336573861671\n",
      "308.24039033298277\n",
      "Outlier clipping complete using IQR method.\n",
      "Final file saved using IQR clipping.\n"
     ]
    }
   ],
   "source": [
    "data_path=Path(\"../Data/cleaned_food_data.csv\")\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# --- STEP 1: Outlier Clipping using YOUR IQR Method ---\n",
    "numeric_cols = ['Calories', 'Protein', 'Fat', 'Carbs', 'Sugar', 'Fiber', \n",
    "                'Sodium', 'Cholesterol', 'Glycemic_Index', 'Water_Content', 'Serving_Size']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    print(lower_bound)\n",
    "    print(upper_bound)\n",
    "    # Clip the data using the IQR bounds\n",
    "    df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "print(\"Outlier clipping complete using IQR method.\")\n",
    "\n",
    "# --- STEP 2: Feature Engineering (Rest of the steps) ---\n",
    "df['Is_Vegan'] = df['Is_Vegan'].astype(int)\n",
    "df['Is_Gluten_Free'] = df['Is_Gluten_Free'].astype(int)\n",
    "\n",
    "df = pd.get_dummies(df, columns=['Meal_Type', 'Preparation_Method'], prefix=['Meal', 'Prep'])\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['Food_Name_Encoded'] = le.fit_transform(df['Food_Name'])\n",
    "\n",
    "# Scaling is still needed for Logistic/KNN/SVM\n",
    "scaler = StandardScaler()\n",
    "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "df_final = df.drop(columns=['Food_Name'])\n",
    "df_final.to_csv('../Data/feature_engineered_data.csv', index=False)\n",
    "print(\"Final file saved using IQR clipping.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
  },
  {
   "cell_type": "markdown",
   "id": "feature_selection_header",
   "metadata": {},
   "source": [
    "## Phase 1: PCA and Feature Selection\n",
    "Adding dimensionality reduction and feature selection techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pca_feature_selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the feature engineered data\n",
    "df_features = pd.read_csv('../Data/feature_engineered_data.csv')\n",
    "print(f\"Original dataset shape: {df_features.shape}\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df_features.drop('Food_Name_Encoded', axis=1)\n",
    "y = df_features['Food_Name_Encoded']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# --- STEP 1: Remove Highly Correlated Features ---\n",
    "print(\"\\n=== Removing Highly Correlated Features ===\")\n",
    "correlation_matrix = X.corr().abs()\n",
    "upper_triangle = correlation_matrix.where(\n",
    "    np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)\n",
    ")\n",
    "\n",
    "# Find features with correlation > 0.95\n",
    "high_corr_features = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.95)]\n",
    "print(f\"Highly correlated features to remove: {high_corr_features}\")\n",
    "\n",
    "# Remove highly correlated features\n",
    "X_reduced = X.drop(columns=high_corr_features)\n",
    "print(f\"Shape after removing correlated features: {X_reduced.shape}\")\n",
    "\n",
    "# --- STEP 2: Feature Selection using SelectKBest ---\n",
    "print(\"\\n=== Feature Selection using SelectKBest ===\")\n",
    "# Select top 20 features using chi-squared test\n",
    "k_best = min(20, X_reduced.shape[1])  # Don't select more features than available\n",
    "selector = SelectKBest(score_func=chi2, k=k_best)\n",
    "\n",
    "# Make sure all values are non-negative for chi2 test\n",
    "X_positive = X_reduced - X_reduced.min() + 1e-6\n",
    "X_selected = selector.fit_transform(X_positive, y)\n",
    "\n",
    "# Get selected feature names\n",
    "selected_features = X_reduced.columns[selector.get_support()].tolist()\n",
    "print(f\"Selected {len(selected_features)} features: {selected_features}\")\n",
    "print(f\"Shape after feature selection: {X_selected.shape}\")\n",
    "\n",
    "# --- STEP 3: PCA Implementation ---\n",
    "print(\"\\n=== PCA Implementation ===\")\n",
    "# Apply PCA to retain 95% of variance\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "X_pca = pca.fit_transform(X_selected)\n",
    "\n",
    "print(f\"Original features: {X_selected.shape[1]}\")\n",
    "print(f\"PCA components: {X_pca.shape[1]}\")\n",
    "print(f\"Explained variance ratio: {pca.explained_variance_ratio_.sum():.4f}\")\n",
    "\n",
    "# --- STEP 4: Visualization ---\n",
    "print(\"\\n=== Creating Visualizations ===\")\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Feature importance from SelectKBest\n",
    "plt.subplot(1, 3, 1)\n",
    "feature_scores = selector.scores_[selector.get_support()]\n",
    "plt.barh(range(len(selected_features)), feature_scores)\n",
    "plt.yticks(range(len(selected_features)), selected_features, fontsize=8)\n",
    "plt.xlabel('Chi-squared Score')\n",
    "plt.title('Feature Importance (SelectKBest)')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Plot 2: PCA explained variance\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "         np.cumsum(pca.explained_variance_ratio_), 'bo-')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('PCA Explained Variance')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot 3: Correlation heatmap of selected features\n",
    "plt.subplot(1, 3, 3)\n",
    "selected_df = pd.DataFrame(X_selected, columns=selected_features)\n",
    "correlation_selected = selected_df.corr()\n",
    "sns.heatmap(correlation_selected, annot=False, cmap='coolwarm', center=0, \n",
    "            square=True, cbar_kws={'shrink': 0.8})\n",
    "plt.title('Correlation Matrix (Selected Features)')\n",
    "plt.xticks(rotation=45, ha='right', fontsize=6)\n",
    "plt.yticks(rotation=0, fontsize=6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- STEP 5: Save Different Versions of Data ---\n",
    "print(\"\\n=== Saving Processed Datasets ===\")\n",
    "\n",
    "# Save original feature engineered data (already saved)\n",
    "print(\"✓ Original feature engineered data already saved\")\n",
    "\n",
    "# Save selected features data\n",
    "df_selected = pd.DataFrame(X_selected, columns=selected_features)\n",
    "df_selected['Food_Name_Encoded'] = y.values\n",
    "df_selected.to_csv('../Data/feature_selected_data.csv', index=False)\n",
    "print(f\"✓ Feature selected data saved: {df_selected.shape}\")\n",
    "\n",
    "# Save PCA data\n",
    "pca_columns = [f'PC{i+1}' for i in range(X_pca.shape[1])]\n",
    "df_pca = pd.DataFrame(X_pca, columns=pca_columns)\n",
    "df_pca['Food_Name_Encoded'] = y.values\n",
    "df_pca.to_csv('../Data/feature_pca_data.csv', index=False)\n",
    "print(f\"✓ PCA data saved: {df_pca.shape}\")\n",
    "\n",
    "# Save feature names for later use\n",
    "feature_info = {\n",
    "    'original_features': X.columns.tolist(),\n",
    "    'selected_features': selected_features,\n",
    "    'pca_components': pca_columns,\n",
    "    'removed_correlated': high_corr_features\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('../Data/feature_info.json', 'w') as f:\n",
    "    json.dump(feature_info, f, indent=2)\n",
    "print(\"✓ Feature information saved\")\n",
    "\n",
    "print(\"\\n=== Summary ===\")\n",
    "print(f\"Original features: {len(X.columns)}\")\n",
    "print(f\"After removing correlated: {len(X_reduced.columns)}\")\n",
    "print(f\"After feature selection: {len(selected_features)}\")\n",
    "print(f\"PCA components: {len(pca_columns)}\")\n",
    "print(f\"Explained variance: {pca.explained_variance_ratio_.sum():.4f}\")"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5
}